{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is better, sym or antisym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "\n",
    "from skimage.filters import sobel_h\n",
    "from skimage.filters import sobel_v\n",
    "from scipy import stats\n",
    "\n",
    "#from sa_decomp_layer import SADecompLayer\n",
    "\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"  #disables GPU \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#tf.__version__\n",
    "from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10,10]\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import decode_predictions\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "from tensorflow.nn import depthwise_conv2d\n",
    "from tensorflow.math import multiply, reduce_sum, reduce_euclidean_norm, sin, cos, abs\n",
    "from tensorflow import stack, concat, expand_dims\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.data import Dataset, AUTOTUNE\n",
    "from imagenet_utils.imagenet_clsloc2 import clsloc\n",
    "from imagenet_utils.load_images import load_images\n",
    "from imagenet_utils.preprocess import preprocess\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_min_or_max(tensor, axis=None):\n",
    "    max = tf.reduce_max(tensor, axis=axis)\n",
    "    min = tf.reduce_min(tensor, axis=axis)\n",
    "    k = tf.where(tf.math.abs(min) > max, x=-1., y=1.)\n",
    "    return tf.math.multiply(tf.reduce_max(tf.math.abs(tensor), axis=axis),  k) \n",
    "\n",
    "def mean_direction(angles):\n",
    "    angles = (angles)\n",
    "    n = angles.shape[0]\n",
    "    print(n)\n",
    "\n",
    "    C = np.sum(np.cos(angles))\n",
    "    S = np.sum(np.sin(angles))\n",
    "    print(C,S)\n",
    "    if S > 0 and C > 0:\n",
    "        return (np.arctan2(S,C))\n",
    "    if C < 0:\n",
    "        return (np.arctan2(S,C) + np.pi )\n",
    "    if S<0 and C > 0:\n",
    "        print(1)\n",
    "        return (np.arctan2(S,C) + 2*np.pi)\n",
    "\n",
    "def get_conv_layer(model, layer):\n",
    "    conv_layers = []\n",
    "    for l in model.layers:\n",
    "        if 'conv2d' in str(type(l)).lower():\n",
    "            conv_layers.append(l)\n",
    "    layer = conv_layers[layer]\n",
    "    return layer\n",
    "\n",
    "def get_filter_and_bias(model, layer):\n",
    "\n",
    "    conv_layers = []\n",
    "    for l in model.layers:\n",
    "        if 'conv2d' in str(type(l)).lower():\n",
    "            conv_layers.append(l)\n",
    "    layer = conv_layers[layer]\n",
    "\n",
    "    # check for convolutional layer\n",
    "    if 'conv' not in layer.name:\n",
    "        raise ValueError('Layer must be a conv. layer')\n",
    "    # get filter weights\n",
    "    filters, biases = layer.get_weights()\n",
    "    #print(\"biases shape : \", biases.shape)\n",
    "    #print(\"filters shape : \", filters.shape)\n",
    "\n",
    "    return (filters, biases)\n",
    "    #print(layer.name, filters.shape)\n",
    "\n",
    "# I use sobel to determine the dominant orientation of the filter\n",
    "def getSobelAngle(f):\n",
    "\n",
    "    s_h = sobel_h(f)\n",
    "    s_v = sobel_v(f)\n",
    "\n",
    "    return (np.arctan2(s_h,s_v))\n",
    "\n",
    "def getSobelTF(f):\n",
    "\n",
    "    sobel_h = np.array([[1., 2., 1.], [0., 0., 0.], [-1., -2., -1.]], dtype=np.float32).reshape((3, 3, 1, 1) )/-4\n",
    "    sobel_v = np.array([[1., 0., -1.], [2., 0., -2.], [1., 0., -1.]], dtype=np.float32).reshape((3, 3, 1, 1))/-4    \n",
    "\n",
    "    s_h = reduce_sum(multiply(f, sobel_h), axis=[0,1])\n",
    "    s_v = reduce_sum(multiply(f, sobel_v), axis=[0,1])\n",
    "\n",
    "    return (np.arctan2(s_h,s_v))\n",
    "\n",
    "\n",
    "\n",
    "def getSymAntiSym(filter):\n",
    "\n",
    "    #patches = extract_image_patches(filters, [1, k, k, 1],  [1, k, k, 1], rates = [1,1,1,1] , padding = 'VALID')\n",
    "    #print(patches)\n",
    "    mat_flip_x = np.fliplr(filter)\n",
    "\n",
    "    mat_flip_y = np.flipud(filter)\n",
    "\n",
    "    mat_flip_xy =  np.fliplr( np.flipud(filter))\n",
    "\n",
    "    sum = filter + mat_flip_x + mat_flip_y + mat_flip_xy\n",
    "    mat_sum_rot_90 = np.rot90(sum)\n",
    "    \n",
    "    return  (sum + mat_sum_rot_90) / 8, filter - ((sum + mat_sum_rot_90) / 8)\n",
    "\n",
    "def getSymAntiSymTF(filter):\n",
    "\n",
    "    #patches = extract_image_patches(filters, [1, k, k, 1],  [1, k, k, 1], rates = [1,1,1,1] , padding = 'VALID')\n",
    "    #print(patches)\n",
    "    a = filter[0,0,:,:]\n",
    "    b = filter[0,1,:,:]\n",
    "    c = filter[0,2,:,:]\n",
    "    d = filter[1,0,:,:]\n",
    "    e = filter[1,1,:,:]\n",
    "    f = filter[1,2,:,:]\n",
    "    g = filter[2,0,:,:]\n",
    "    h = filter[2,1,:,:]\n",
    "    i = filter[2,2,:,:]\n",
    "\n",
    "    fs1 = expand_dims(a+c+g+i, 0)/4\n",
    "    fs2 = expand_dims(b+d+f+h,0)/4\n",
    "    fs3= expand_dims(e, 0)\n",
    "\n",
    "    sym = stack([concat([fs1, fs2, fs1],  axis=0), \n",
    "                         concat([fs2, fs3, fs2], axis=0),\n",
    "                         concat([fs1, fs2, fs1], axis=0)])\n",
    "        \n",
    "    anti = filter - sym\n",
    "\n",
    "    return sym, anti\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment #1 : Anistym only in block5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262144"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512*512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BETA tf.Tensor(0.92020845, shape=(), dtype=float32)\n",
      "BETA tf.Tensor(0.31845495, shape=(), dtype=float32)\n",
      "BETA tf.Tensor(0.8186765, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "model = VGG16(weights='imagenet',\n",
    "\t\t\t\t  include_top=True,\n",
    "\t\t\t\t  input_shape=(224, 224, 3))\n",
    "conv_layers = []\n",
    "for l in model.layers:\n",
    "\tif 'conv2d' in str(type(l)).lower():\n",
    "\t\tif l.kernel_size == (3,3):\n",
    "\t\t\tconv_layers.append(l)\n",
    "\n",
    "for layer in conv_layers[-3:]:\n",
    "\t#layer = get_conv_layer(model, -3)\n",
    "\tfilters, biases = layer.get_weights()\n",
    "\tsym, antisym = getSymAntiSymTF(filters)\n",
    "\ta_energy = reduce_euclidean_norm(antisym, axis=[0,1])**2\n",
    "\ts_energy = reduce_euclidean_norm(sym, axis=[0,1])**2\n",
    "\ttotal_energy  = reduce_euclidean_norm(filters, axis=[0,1])**2\n",
    "\tbeta  = a_energy/total_energy\n",
    "\tprint(\"BETA\", beta[0,0])\n",
    "\n",
    "\tmixed_indicies = tf.where(tf.math.logical_and(beta > 0.3 , beta < 0.7 ))\n",
    "\t#r = tf.sqrt(tf.reduce_sum(tf.square(filters), axis=[0,1]))  \n",
    "\t#norm = tf.sqrt(tf.reduce_sum(tf.square(filters), axis=[0,1]))  \n",
    "\n",
    "\t#filters[:,:,mixed_indicies.numpy()[:,0],mixed_indicies.numpy()[:, 1]] = np.zeros((3,3,1))\t\n",
    "\tlayer.set_weights([filters, biases])\n",
    "\n",
    "\t'''layer = get_conv_layer(model, -2)\n",
    "\tfilters, biases = layer.get_weights()\n",
    "\tsym, antisym = getSymAntiSymTF(filters)\n",
    "\tr = tf.sqrt(tf.reduce_sum(tf.square(filters), axis=[0,1]))  \n",
    "\tnorm = tf.sqrt(tf.reduce_sum(tf.square(filters), axis=[0,1]))  \n",
    "\tf = tf.math.multiply((filters / norm) , r)\n",
    "\tlayer.set_weights([f, biases])\n",
    "\n",
    "\tlayer = get_conv_layer(model, -1)\n",
    "\tfilters, biases = layer.get_weights()\n",
    "\tsym, antisym = getSymAntiSymTF(filters)\n",
    "\tr = tf.sqrt(tf.reduce_sum(tf.square(filters), axis=[0,1]))  \n",
    "\tnorm = tf.sqrt(tf.reduce_sum(tf.square(filters), axis=[0,1]))  \n",
    "\tf = tf.math.multiply((filters / norm) , r)\n",
    "\tlayer.set_weights([f, biases])'''\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "\t\t\t  metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = filters[0,0,:,:]\n",
    "b = filters[0,1,:,:]\n",
    "c = filters[0,2,:,:]\n",
    "d = filters[1,0,:,:]\n",
    "e = filters[1,1,:,:]\n",
    "f = filters[1,2,:,:]\n",
    "g = filters[2,0,:,:]\n",
    "h = filters[2,1,:,:]\n",
    "i = filters[2,2,:,:]\n",
    "\n",
    "fs1 = (a+c+g+i)/4\n",
    "fs2 = (b+d+f+h)/4\n",
    "fs3= e\n",
    "\n",
    "\n",
    "total_e = tf.math.reduce_euclidean_norm(filters, axis=[0,1])**2\n",
    "sym_e= 4*fs1**2. + 4*fs2**2. + fs3**2. \n",
    "anti_e = total_e - sym_e\n",
    "beta = anti_e / total_e\n",
    "dist = (beta - 0.5)**2\n",
    "reg = 1-4*dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(512, 512), dtype=float32, numpy=\n",
       "array([[0.8186765 , 0.22181098, 0.01863944, ..., 0.0069037 , 0.8516732 ,\n",
       "        0.99361306],\n",
       "       [0.6345305 , 0.6140087 , 0.0287166 , ..., 0.8402725 , 0.8358974 ,\n",
       "        0.57847977],\n",
       "       [0.20562099, 0.938982  , 0.36317477, ..., 0.35349146, 0.4544615 ,\n",
       "        0.49786338],\n",
       "       ...,\n",
       "       [0.46263227, 0.6881922 , 0.9081367 , ..., 0.17629305, 0.7439231 ,\n",
       "        0.04354893],\n",
       "       [0.4872579 , 0.9763647 , 0.46864447, ..., 0.8667311 , 0.8501377 ,\n",
       "        0.34095222],\n",
       "       [0.3247266 , 0.2211252 , 0.25607044, ..., 0.14881541, 0.24192235,\n",
       "        0.85003597]], dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagenet Validation Set location\n",
    "img = \"C:/ILSVRC2012_img_val/ILSVRC2012_val_00000028.JPEG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = glob(\"C:/imagenette2/train/*/*.JPEG\")\n",
    "train_labels = glob(\"C:/imagenette2/train/*/*.JPEG\")\n",
    "train_labels = to_categorical([clsloc[os.path.normpath(str(path)).split(os.path.sep)[-2]] for path in train_labels], num_classes = 1000)\n",
    "\n",
    "\n",
    "trainDS = Dataset.from_tensor_slices((train_list, train_labels))\n",
    "trainDS = (trainDS\n",
    "\t.map(load_images)\n",
    "\t.map(preprocess)\n",
    "\t.cache()\n",
    "\t.batch(20)\n",
    "\t.prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "\n",
    "val_list = glob(\"C:/imagenette2/val/*/*.JPEG\")\n",
    "val_labels = glob(\"C:/imagenette2/val/*/*.JPEG\")\n",
    "val_labels = to_categorical([clsloc[os.path.normpath(str(path)).split(os.path.sep)[-2]] for path in val_list], num_classes = 1000)\n",
    "\n",
    "\n",
    "valDS = Dataset.from_tensor_slices((val_list, val_labels))\n",
    "valDS = (valDS\n",
    "\t.map(load_images)\n",
    "\t.map(preprocess)\n",
    "\t.cache()\n",
    "\t.batch(20)\n",
    "\t.prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(train_labels[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 8s 169ms/step - loss: 2.8276 - accuracy: 0.4650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.8275744915008545, 0.4650000035762787]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valDS.take(40))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
