{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tYAj4FcLKGO"
   },
   "source": [
    "# Visualizing what convnets learn\n",
    "\n",
    "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
    "**Date created:** 2020/05/29<br>\n",
    "**Last modified:** 2020/05/29<br>\n",
    "**Description:** Displaying the visual patterns that convnet filters respond to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Imit-ugbLKGT"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this example, we look into what sort of visual patterns image classification models\n",
    "learn. We'll be using the `ResNet50V2` model, trained on the ImageNet dataset.\n",
    "\n",
    "Our process is simple: we will create input images that maximize the activation of\n",
    "specific filters in a target layer (picked somewhere in the middle of the model: layer\n",
    "`conv3_block4_out`). Such images represent a visualization of the\n",
    "pattern that the filter responds to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooPMQE7VLKGT"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TkHTiDsALKGV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# The dimensions of our input image\n",
    "img_width = 224\n",
    "img_height = 224\n",
    "# Our target layer: we will visualize the filters from this layer.\n",
    "# See `model.summary()` for list of layer names, if you want to change this.\n",
    "layer_name = \"block5_conv1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRMHrNI6LKGX"
   },
   "source": [
    "## Build a feature extraction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DGMOYCsrLKGY"
   },
   "outputs": [],
   "source": [
    "# Build a ResNet50V2 model loaded with pre-trained ImageNet weights\n",
    "model = tf.keras.applications.VGG16(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "# Set up a model that returns the activation values for our target layer\n",
    "layer = model.get_layer(name=layer_name)\n",
    "feature_extractor = keras.Model(inputs=model.inputs, outputs=layer.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6kuqWNuLKGY"
   },
   "source": [
    "## Set up the gradient ascent process\n",
    "\n",
    "The \"loss\" we will maximize is simply the mean of the activation of a specific filter in\n",
    "our target layer. To avoid border effects, we exclude border pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "o38T1ry3LKGZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_loss(input_image, filter_index):\n",
    "    activation = feature_extractor(input_image)\n",
    "    # We avoid border artifacts by only involving non-border pixels in the loss.\n",
    "    filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n",
    "    return tf.reduce_mean(filter_activation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqNRwbZcLKGZ"
   },
   "source": [
    "Our gradient ascent function simply computes the gradients of the loss above\n",
    "with regard to the input image, and update the update image so as to move it\n",
    "towards a state that will activate the target filter more strongly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "id": "59QhJAP-LKGa"
   },
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def gradient_ascent_step(img, filter_index, learning_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(img)\n",
    "        loss = compute_loss(img, filter_index)\n",
    "    # Compute gradients.\n",
    "    grads = tape.gradient(loss, img)\n",
    "    # Normalize gradients.\n",
    "    grads = tf.math.l2_normalize(grads)\n",
    "    img += learning_rate * grads + (1.5e-6)*tf.math.reduce_euclidean_norm(img)**2\n",
    "    return loss, img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HfQCC-2LKGa"
   },
   "source": [
    "## Set up the end-to-end filter visualization loop\n",
    "\n",
    "Our process is as follow:\n",
    "\n",
    "- Start from a random image that is close to \"all gray\" (i.e. visually netural)\n",
    "- Repeatedly apply the gradient ascent step function defined above\n",
    "- Convert the resulting input image back to a displayable form, by normalizing it,\n",
    "center-cropping it, and restricting it to the [0, 255] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "id": "jNQqiVMZLKGa"
   },
   "outputs": [],
   "source": [
    "\n",
    "def initialize_image():\n",
    "    # We start from a gray image with some random noise\n",
    "    img = tf.random.uniform((1, img_width, img_height, 3))\n",
    "    # ResNet50V2 expects inputs in the range [-1, +1].\n",
    "    # Here we scale our random inputs to [-0.125, +0.125]\n",
    "    return (img - 0.5) \n",
    "\n",
    "\n",
    "def visualize_filter(filter_index):\n",
    "    # We run gradient ascent for 20 steps\n",
    "    iterations = 30\n",
    "    learning_rate = 50\n",
    "    img = initialize_image()\n",
    "    for iteration in range(iterations):\n",
    "        loss, img = gradient_ascent_step(img, filter_index, learning_rate)\n",
    "\n",
    "    # Decode the resulting input image\n",
    "    img = deprocess_image(img[0].numpy())\n",
    "    return loss, img\n",
    "\n",
    "\n",
    "def deprocess_image(img):\n",
    "    # Normalize array: center on 0., ensure variance is 0.15\n",
    "    img -= img.mean()\n",
    "    img /= img.std() + 1e-5\n",
    "    img *= 0.15\n",
    "\n",
    "    # Center crop\n",
    "    img = img[25:-25, 25:-25, :]\n",
    "\n",
    "    # Clip to [0, 1]\n",
    "    img += 0.5\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    # Convert to RGB array\n",
    "    img *= 255\n",
    "    img = np.clip(img, 0, 255).astype(\"uint8\")\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lZIXWP1LKGb"
   },
   "source": [
    "Let's try it out with filter 0 in the target layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "id": "GT1VjpC-LKGb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AN46710\\Anaconda3\\envs\\masters\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "loss, img = visualize_filter(61)\n",
    "keras.utils.save_img(\"0.png\", img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qb30U93ELKGb"
   },
   "source": [
    "This is what an input that maximizes the response of filter 0 in the target layer would\n",
    "look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174, 174)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img[...,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    0.,     0.,     0.,     0.,     0., 90828.,     0.,     0.,\n",
       "            0.,     0.]),\n",
       " array([-0.5, -0.4, -0.3, -0.2, -0.1,  0. ,  0.1,  0.2,  0.3,  0.4,  0.5]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(tf.reshape(img,-1).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "id": "EYGmtuOwLKGb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK4AAACuCAIAAAAgbqG5AAAAb0lEQVR4nO3BAQ0AAADCoPdPbQ43oAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABeDWOJAAH8b1/4AAAAAElFTkSuQmCC",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(\"0.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwyxRyk4LKGc"
   },
   "source": [
    "## Visualize the first 64 filters in the target layer\n",
    "\n",
    "Now, let's make a 8x8 grid of the first 64 filters\n",
    "in the target layer to get of feel for the range\n",
    "of different visual patterns that the model has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9Uli6CPBLKGc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing filter 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21428\\3495890723.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfilter_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Processing filter %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfilter_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvisualize_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mall_imgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21428\\2445521896.py\u001b[0m in \u001b[0;36mvisualize_filter\u001b[1;34m(filter_index)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitialize_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_ascent_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# Decode the resulting input image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\AN46710\\Anaconda3\\envs\\masters\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\AN46710\\Anaconda3\\envs\\masters\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 898\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\AN46710\\Anaconda3\\envs\\masters\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    935\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 937\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    938\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mc:\\Users\\AN46710\\Anaconda3\\envs\\masters\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2468\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2469\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2470\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2472\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\AN46710\\Anaconda3\\envs\\masters\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1834\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1835\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1836\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1837\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1838\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\AN46710\\Anaconda3\\envs\\masters\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    483\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 485\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    486\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\Users\\AN46710\\Anaconda3\\envs\\masters\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 55\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compute image inputs that maximize per-filter activations\n",
    "# for the first 64 filters of our target layer\n",
    "all_imgs = []\n",
    "for filter_index in range(64):\n",
    "    print(\"Processing filter %d\" % (filter_index,))\n",
    "    loss, img = visualize_filter(filter_index)\n",
    "    all_imgs.append(img)\n",
    "\n",
    "# Build a black picture with enough space for\n",
    "# our 8 x 8 filters of size 128 x 128, with a 5px margin in between\n",
    "margin = 5\n",
    "n = 8\n",
    "cropped_width = img_width - 25 * 2\n",
    "cropped_height = img_height - 25 * 2\n",
    "width = n * cropped_width + (n - 1) * margin\n",
    "height = n * cropped_height + (n - 1) * margin\n",
    "stitched_filters = np.zeros((width, height, 3))\n",
    "\n",
    "# Fill the picture with our saved filters\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        img = all_imgs[i * n + j]\n",
    "        stitched_filters[\n",
    "            (cropped_width + margin) * i : (cropped_width + margin) * i + cropped_width,\n",
    "            (cropped_height + margin) * j : (cropped_height + margin) * j\n",
    "            + cropped_height,\n",
    "            :,\n",
    "        ] = img\n",
    "keras.utils.save_img(\"stiched_filters.png\", stitched_filters)\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(\"stiched_filters.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oezhqd3DLKGc"
   },
   "source": [
    "Image classification models see the world by decomposing their inputs over a \"vector\n",
    "basis\" of texture filters such as these.\n",
    "\n",
    "See also\n",
    "[this old blog post](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html)\n",
    "for analysis and interpretation."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "visualizing_what_convnets_learn",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
