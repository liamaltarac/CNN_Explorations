{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "\n",
    "from skimage.filters import sobel_h\n",
    "from skimage.filters import sobel_v\n",
    "from scipy import stats\n",
    "\n",
    "#from sa_decomp_layer import SADecompLayer\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "#tf.__version__\n",
    "from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10,10]\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import decode_predictions\n",
    "from tensorflow.keras.applications import VGG16, ResNet152, ResNet50\n",
    "\n",
    "from tensorflow.nn import depthwise_conv2d\n",
    "from tensorflow.math import multiply, reduce_sum, reduce_euclidean_norm, sin, cos, abs\n",
    "from tensorflow import stack, concat, expand_dims\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.data import Dataset, AUTOTUNE\n",
    "from imagenet_utils.imagenet_clsloc2 import clsloc\n",
    "from imagenet_utils.load_images import load_images\n",
    "from imagenet_utils.preprocess import preprocess\n",
    "\n",
    "from utils.utils import *\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import scienceplots\n",
    "\n",
    "plt.style.use(['science', 'ieee'])\n",
    "plt.rcParams.update({'figure.dpi': '600'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "model = VGG16(weights='imagenet',\n",
    "\t\t\t\t  include_top=True,\n",
    "\t\t\t\t  input_shape=(224, 224, 3))\n",
    "\n",
    "model.compile(loss=keras.losses.CategoricalCrossentropy(from_logits=False),            \n",
    "\t\t\tmetrics=[\n",
    "\t\t\t\tkeras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "\t\t\t\tkeras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate (model_original, layers)  :\n",
    "\tmodel = tf.keras.models.clone_model(model_original)\n",
    "\tmodel.set_weights(model_original.get_weights())\n",
    "\tconv_layers = []\n",
    "\tfor l in model.layers:\n",
    "\t\tif 'conv2d' in str(type(l)).lower():\n",
    "\t\t\tif l.kernel_size == (3,3):\n",
    "\t\t\t\tconv_layers.append(l)\n",
    "\t\t\t\tprint(l)\n",
    "\tfor l in layers:\n",
    "\t\tfilters, biases = conv_layers[l].get_weights()\n",
    "\n",
    "\t\t# Reshape weights to (64 * 128, 3, 3) for batch processing\n",
    "\t\tweights_reshaped = tf.reshape(filters, (3, 3,-1))\n",
    "\n",
    "\t\trotated_weights = tf.image.rot90(weights_reshaped, k=2)\n",
    "\t\trotated_weights = tf.reshape(rotated_weights, (3, 3,filters.shape[-2],filters.shape[-1]))\n",
    "\t\t\n",
    "\t\tconv_layers[l].set_weights([rotated_weights, biases])\n",
    "\n",
    "\n",
    "\treturn model\n",
    "\n",
    "\t\t\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagenet Validation Set location\n",
    "img = \"C:/ILSVRC2012_img_val/ILSVRC2012_val_00000028.JPEG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = glob(\"C:/imagenette2/train/*/*.JPEG\")\n",
    "train_labels = glob(\"C:/imagenette2/train/*/*.JPEG\")\n",
    "train_labels = to_categorical([clsloc[os.path.normpath(str(path)).split(os.path.sep)[-2]] for path in train_labels], num_classes = 1000)\n",
    "\n",
    "\n",
    "trainDS = Dataset.from_tensor_slices((train_list, train_labels))\n",
    "trainDS = (trainDS\n",
    "\t.map(load_images)\n",
    "\t.map(preprocess)\n",
    "\t.cache()\n",
    "\t.batch(1)\n",
    "\t.prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "\n",
    "val_list = glob(\"C:/imagenette2/val/*/*.JPEG\")\n",
    "val_labels = glob(\"C:/imagenette2/val/*/*.JPEG\")\n",
    "val_labels = to_categorical([clsloc[os.path.normpath(str(path)).split(os.path.sep)[-2]] for path in val_list], num_classes = 1000)\n",
    "\n",
    "\n",
    "valDS = Dataset.from_tensor_slices((val_list, val_labels))\n",
    "valDS = (valDS\n",
    "\t.map(load_images)\n",
    "\t.map(preprocess)\n",
    "\t.cache()\n",
    "\t.batch(32)\n",
    "\t.prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3925, 1000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000022348A09408>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000002237FD15A48>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000022304B17AC8>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000002236C4B3848>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000002236C4B4A08>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000002236C4B84C8>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000002236C4B8F08>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000002236C4DE5C8>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000002236C4BC108>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000002236C4BCB88>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000002236C4BFCC8>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000002236C4BE788>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000002236C4C2288>\n"
     ]
    }
   ],
   "source": [
    "model_rot = rotate(model, [-1])\n",
    "model_rot.compile(loss=keras.losses.CategoricalCrossentropy(from_logits=False),            \n",
    "\t\t\tmetrics=[\n",
    "\t\t\t\tkeras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "\t\t\t\tkeras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 [==============================] - 143s 1s/step - loss: 0.8546 - accuracy: 0.7720 - top-5-accuracy: 0.9386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8546394109725952, 0.771974503993988, 0.9385987520217896]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rot.evaluate(valDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 [==============================] - 16s 113ms/step - loss: 0.6894 - accuracy: 0.8061 - top-5-accuracy: 0.9549\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6894471049308777, 0.806114673614502, 0.9549044370651245]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valDS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
