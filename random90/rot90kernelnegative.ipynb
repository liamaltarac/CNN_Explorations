{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "\n",
    "from skimage.filters import sobel_h\n",
    "from skimage.filters import sobel_v\n",
    "from scipy import stats\n",
    "\n",
    "#from sa_decomp_layer import SADecompLayer\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "#tf.__version__\n",
    "from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10,10]\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import decode_predictions\n",
    "from tensorflow.keras.applications import VGG16, ResNet152, ResNet50\n",
    "\n",
    "from tensorflow.nn import depthwise_conv2d\n",
    "from tensorflow.math import multiply, reduce_sum, reduce_euclidean_norm, sin, cos, abs\n",
    "from tensorflow import stack, concat, expand_dims\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.data import Dataset, AUTOTUNE\n",
    "from imagenet_utils.imagenet_clsloc2 import clsloc\n",
    "from imagenet_utils.load_images import load_images\n",
    "from imagenet_utils.preprocess import preprocess\n",
    "\n",
    "from utils.utils import *\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import scienceplots\n",
    "\n",
    "plt.style.use(['science', 'ieee'])\n",
    "plt.rcParams.update({'figure.dpi': '600'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000016478D307C8>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000016478B962C8>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x00000162DBCA2888>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x00000162DBCA6CC8>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000016477C9BAC8>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x00000162DED86708>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x00000162DBCC4E48>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000016477CD8488>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x00000162DED85C08>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x00000162DED39908>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000016478BF68C8>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000016477CD5448>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x00000162CC318708>\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "model = VGG16(weights='imagenet',\n",
    "\t\t\t\t  include_top=True,\n",
    "\t\t\t\t  input_shape=(224, 224, 3))\n",
    "\n",
    "model.compile(loss=keras.losses.CategoricalCrossentropy(from_logits=False),            \n",
    "\t\t\tmetrics=[\n",
    "\t\t\t\tkeras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "\t\t\t\tkeras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),])\n",
    "\n",
    "\n",
    "conv_layers = []\n",
    "for l in model.layers:\n",
    "\tif 'conv2d' in str(type(l)).lower():\n",
    "\t\tif l.kernel_size == (3,3) or l.kernel_size == (7,7):\n",
    "\t\t\tconv_layers.append(l)\n",
    "\t\t\tprint(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate (model_original, layers)  :\n",
    "\tmodel = tf.keras.models.clone_model(model_original)\n",
    "\tmodel.set_weights(model_original.get_weights())\n",
    "\tconv_layers = []\n",
    "\tfor l in model.layers:\n",
    "\t\tif 'conv2d' in str(type(l)).lower():\n",
    "\t\t\tif l.kernel_size == (3,3) or l.kernel_size == (7,7):\n",
    "\t\t\t\tconv_layers.append(l)\n",
    "\t\t\t\tprint(l)\n",
    "\tfor l in layers:\n",
    "\t\tfilters, biases = conv_layers[l].get_weights()\n",
    "\n",
    "\n",
    "\t\t# Generate random rotations (0, 1, 2, 3 corresponding to 0째, 90째, 180째, 270째)\n",
    "\t\trandom_rotations = tf.random.uniform((1,1,filters.shape[-2]*filters.shape[-1]), minval=2, maxval=3, dtype=tf.int32) * tf.ones((3,3,filters.shape[-2]*filters.shape[-1]), dtype=tf.int32)\n",
    "\n",
    "\t\t# Reshape weights to (64 * 128, 3, 3) for batch processing\n",
    "\t\tweights_reshaped = tf.reshape(filters, (3, 3,-1))\n",
    "\t\trandom_rotations *= tf.where(tf.less(tf.reduce_mean(weights_reshaped, axis=(0,1)), 0), x=0, y=1)\n",
    "\t\trotation_masks = [tf.equal(random_rotations, k) for k in [0,2]] \n",
    "\t\tprint(random_rotations)\n",
    "\t\t# Apply rotations based on the masks\n",
    "\t\trotated_batches = [\n",
    "\t\t\ttf.image.rot90(weights_reshaped, k=k) * tf.cast(rotation_masks[k//2], tf.float32)\n",
    "\t\t\tfor k in [0,2]\n",
    "\t\t]\n",
    "\n",
    "\t\trotated_weights = tf.add_n(rotated_batches)\n",
    "\t\trotated_weights = tf.reshape(rotated_weights, (3, 3,filters.shape[-2],filters.shape[-1]))\n",
    "\t\t\n",
    "\t\tconv_layers[l].set_weights([rotated_weights, biases])\n",
    "\n",
    "\n",
    "\treturn model\n",
    "\n",
    "\t\t\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagenet Validation Set location\n",
    "img = \"C:/ILSVRC2012_img_val/ILSVRC2012_val_00000028.JPEG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = glob(\"C:/imagenette2/train/*/*.JPEG\")\n",
    "train_labels = glob(\"C:/imagenette2/train/*/*.JPEG\")\n",
    "train_labels = to_categorical([clsloc[os.path.normpath(str(path)).split(os.path.sep)[-2]] for path in train_labels], num_classes = 1000)\n",
    "\n",
    "\n",
    "trainDS = Dataset.from_tensor_slices((train_list, train_labels))\n",
    "trainDS = (trainDS\n",
    "\t.map(load_images)\n",
    "\t.map(preprocess)\n",
    "\t.cache()\n",
    "\t.batch(1)\n",
    "\t.prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "\n",
    "val_list = glob(\"C:/imagenette2/val/*/*.JPEG\")\n",
    "val_labels = glob(\"C:/imagenette2/val/*/*.JPEG\")\n",
    "val_labels = to_categorical([clsloc[os.path.normpath(str(path)).split(os.path.sep)[-2]] for path in val_list], num_classes = 1000)\n",
    "\n",
    "\n",
    "valDS = Dataset.from_tensor_slices((val_list, val_labels))\n",
    "valDS = (valDS\n",
    "\t.map(load_images)\n",
    "\t.map(preprocess)\n",
    "\t.cache()\n",
    "\t.batch(32)\n",
    "\t.prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3925, 1000)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000016478A0C208>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000016478A0A288>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000016488C16B88>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x00000164793178C8>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000016478988648>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000016478982348>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000016478982D08>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000016478984E88>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000016488C34748>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001647897D7C8>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000016478983948>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001647898F3C8>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001647898FDC8>\n",
      "tf.Tensor(\n",
      "[[[0 0 2 ... 0 0 2]\n",
      "  [0 0 2 ... 0 0 2]\n",
      "  [0 0 2 ... 0 0 2]]\n",
      "\n",
      " [[0 0 2 ... 0 0 2]\n",
      "  [0 0 2 ... 0 0 2]\n",
      "  [0 0 2 ... 0 0 2]]\n",
      "\n",
      " [[0 0 2 ... 0 0 2]\n",
      "  [0 0 2 ... 0 0 2]\n",
      "  [0 0 2 ... 0 0 2]]], shape=(3, 3, 262144), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[0 0 0 ... 0 2 2]\n",
      "  [0 0 0 ... 0 2 2]\n",
      "  [0 0 0 ... 0 2 2]]\n",
      "\n",
      " [[0 0 0 ... 0 2 2]\n",
      "  [0 0 0 ... 0 2 2]\n",
      "  [0 0 0 ... 0 2 2]]\n",
      "\n",
      " [[0 0 0 ... 0 2 2]\n",
      "  [0 0 0 ... 0 2 2]\n",
      "  [0 0 0 ... 0 2 2]]], shape=(3, 3, 262144), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[0 2 0 ... 0 0 0]\n",
      "  [0 2 0 ... 0 0 0]\n",
      "  [0 2 0 ... 0 0 0]]\n",
      "\n",
      " [[0 2 0 ... 0 0 0]\n",
      "  [0 2 0 ... 0 0 0]\n",
      "  [0 2 0 ... 0 0 0]]\n",
      "\n",
      " [[0 2 0 ... 0 0 0]\n",
      "  [0 2 0 ... 0 0 0]\n",
      "  [0 2 0 ... 0 0 0]]], shape=(3, 3, 262144), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "model_rot = rotate(model, [-3,-2,-1])\n",
    "model_rot.compile(loss=keras.losses.CategoricalCrossentropy(from_logits=False),            \n",
    "\t\t\tmetrics=[\n",
    "\t\t\t\tkeras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "\t\t\t\tkeras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 [==============================] - 23s 184ms/step - loss: 3.1085 - accuracy: 0.3569 - top-5-accuracy: 0.6318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.10848069190979, 0.35694268345832825, 0.6318471431732178]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rot.evaluate(valDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 [==============================] - 12s 98ms/step - loss: 0.6863 - accuracy: 0.8000 - top-5-accuracy: 0.9605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6863359212875366, 0.800000011920929, 0.9605095386505127]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
