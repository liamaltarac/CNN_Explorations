{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "\n",
    "from skimage.filters import sobel_h\n",
    "from skimage.filters import sobel_v\n",
    "from scipy import stats\n",
    "\n",
    "#from sa_decomp_layer import SADecompLayer\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "#tf.__version__\n",
    "from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10,10]\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import decode_predictions\n",
    "from tensorflow.keras.applications import VGG16, ResNet152, ResNet50\n",
    "\n",
    "from tensorflow.nn import depthwise_conv2d\n",
    "from tensorflow.math import multiply, reduce_sum, reduce_euclidean_norm, sin, cos, abs\n",
    "from tensorflow import stack, concat, expand_dims\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.data import Dataset, AUTOTUNE\n",
    "from imagenet_utils.imagenet_clsloc2 import clsloc\n",
    "from imagenet_utils.load_images import load_images\n",
    "from imagenet_utils.preprocess import preprocess\n",
    "\n",
    "from utils.utils import *\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import scienceplots\n",
    "\n",
    "plt.style.use(['science', 'ieee'])\n",
    "plt.rcParams.update({'figure.dpi': '600'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "model = VGG16(weights='imagenet',\n",
    "\t\t\t\t  include_top=True,\n",
    "\t\t\t\t  input_shape=(224, 224, 3))\n",
    "\n",
    "model.compile(loss=keras.losses.CategoricalCrossentropy(from_logits=False),            \n",
    "\t\t\tmetrics=[\n",
    "\t\t\t\tkeras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "\t\t\t\tkeras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate (model_original)  :\n",
    "\tmodel = tf.keras.models.clone_model(model_original)\n",
    "\tmodel.set_weights(model_original.get_weights())\n",
    "\tconv_layers = []\n",
    "\tfor l in model.layers:\n",
    "\t\tif 'conv2d' in str(type(l)).lower():\n",
    "\t\t\tif l.kernel_size == (3,3):\n",
    "\t\t\t\tconv_layers.append(l)\n",
    "\t\t\t\tprint(l)\n",
    "\tfor l in conv_layers:\n",
    "\t\tprint(l.name)\n",
    "\t\tfilters, biases = l.get_weights()\n",
    "\n",
    "\t\t# Reshape weights to (64 * 128, 3, 3) for batch processing\n",
    "\t\tweights_reshaped = tf.reshape(filters, (3, 3,-1))\n",
    "\n",
    "\t\trotated_weights = tf.image.rot90(weights_reshaped, k=2)\n",
    "\t\trotated_weights = tf.reshape(rotated_weights, (3, 3,filters.shape[-2],filters.shape[-1]))\n",
    "\t\t\n",
    "\t\tl.set_weights([rotated_weights, biases])\n",
    "\n",
    "\n",
    "\treturn model\n",
    "\n",
    "\t\t\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagenet Validation Set location\n",
    "img = \"C:/ILSVRC2012_img_val/ILSVRC2012_val_00000028.JPEG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = glob(\"C:/imagenette2/train/*/*.JPEG\")\n",
    "train_labels = glob(\"C:/imagenette2/train/*/*.JPEG\")\n",
    "train_labels = to_categorical([clsloc[os.path.normpath(str(path)).split(os.path.sep)[-2]] for path in train_labels], num_classes = 1000)\n",
    "\n",
    "\n",
    "trainDS = Dataset.from_tensor_slices((train_list, train_labels))\n",
    "trainDS = (trainDS\n",
    "\t.map(load_images)\n",
    "\t.map(preprocess)\n",
    "\t.cache()\n",
    "\t.batch(1)\n",
    "\t.prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "\n",
    "val_list = glob(\"C:/imagenette2/val/*/*.JPEG\")\n",
    "val_labels = glob(\"C:/imagenette2/val/*/*.JPEG\")\n",
    "val_labels = to_categorical([clsloc[os.path.normpath(str(path)).split(os.path.sep)[-2]] for path in val_list], num_classes = 1000)\n",
    "\n",
    "\n",
    "valDS = Dataset.from_tensor_slices((val_list, val_labels))\n",
    "valDS = (valDS\n",
    "\t.map(load_images)\n",
    "\t.map(preprocess)\n",
    "\t.cache()\n",
    "\t.batch(32)\n",
    "\t.prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3925, 1000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001BDB2731948>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001BDB2734608>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001BDB1B24908>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001BDBF6974C8>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001BDBF696248>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001BDBF6965C8>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001BDBF67C448>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001BDBF66C0C8>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001BDBF66CF48>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001BDB1A97908>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001BDBF6705C8>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001BDBF694E88>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001BDBF6C9F88>\n",
      "block1_conv1\n",
      "block1_conv2\n",
      "block2_conv1\n",
      "block2_conv2\n",
      "block3_conv1\n",
      "block3_conv2\n",
      "block3_conv3\n",
      "block4_conv1\n",
      "block4_conv2\n",
      "block4_conv3\n",
      "block5_conv1\n",
      "block5_conv2\n",
      "block5_conv3\n"
     ]
    }
   ],
   "source": [
    "model_rot = rotate(model)\n",
    "model_rot.compile(loss=keras.losses.CategoricalCrossentropy(from_logits=False),            \n",
    "\t\t\tmetrics=[\n",
    "\t\t\t\tkeras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "\t\t\t\tkeras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 [==============================] - 108s 870ms/step - loss: 2.0530 - accuracy: 0.5248 - top-5-accuracy: 0.7564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0530037879943848, 0.524840772151947, 0.7564331293106079]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rot.evaluate(valDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 [==============================] - 13s 97ms/step - loss: 0.6895 - accuracy: 0.8018 - top-5-accuracy: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6894992589950562, 0.8017834424972534, 0.9599999785423279]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valDS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
