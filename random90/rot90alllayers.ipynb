{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "\n",
    "from skimage.filters import sobel_h\n",
    "from skimage.filters import sobel_v\n",
    "from scipy import stats\n",
    "\n",
    "#from sa_decomp_layer import SADecompLayer\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "#tf.__version__\n",
    "from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10,10]\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import decode_predictions\n",
    "from tensorflow.keras.applications import VGG16, ResNet152, ResNet50\n",
    "\n",
    "from tensorflow.nn import depthwise_conv2d\n",
    "from tensorflow.math import multiply, reduce_sum, reduce_euclidean_norm, sin, cos, abs\n",
    "from tensorflow import stack, concat, expand_dims\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.data import Dataset, AUTOTUNE\n",
    "from imagenet_utils.imagenet_clsloc2 import clsloc\n",
    "from imagenet_utils.load_images import load_images\n",
    "from imagenet_utils.preprocess import preprocess\n",
    "\n",
    "from utils.utils import *\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import scienceplots\n",
    "\n",
    "plt.style.use(['science', 'ieee'])\n",
    "plt.rcParams.update({'figure.dpi': '600'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "model = VGG16(weights='imagenet',\n",
    "\t\t\t\t  include_top=True,\n",
    "\t\t\t\t  input_shape=(224, 224, 3))\n",
    "\n",
    "model.compile(loss=keras.losses.CategoricalCrossentropy(from_logits=False),            \n",
    "\t\t\tmetrics=[\n",
    "\t\t\t\tkeras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "\t\t\t\tkeras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate (model_original, layers, k=2)  :\n",
    "\tmodel = tf.keras.models.clone_model(model_original)\n",
    "\tmodel.set_weights(model_original.get_weights())\n",
    "\tconv_layers = []\n",
    "\tfor l in model.layers:\n",
    "\t\tif 'conv2d' in str(type(l)).lower():\n",
    "\t\t\tif l.kernel_size == (3,3):\n",
    "\t\t\t\tconv_layers.append(l)\n",
    "\t\t\t\tprint(l)\n",
    "\n",
    "\tfor l in layers:\n",
    "\t\tfilters, biases = conv_layers[l].get_weights()\n",
    "\t\t# Reshape weights to (64 * 128, 3, 3) for batch processing\n",
    "\t\tweights_reshaped = tf.reshape(filters, (3, 3,-1))\n",
    "\n",
    "\t\trotated_weights = tf.image.rot90(weights_reshaped, k=k)\n",
    "\t\trotated_weights = tf.reshape(rotated_weights, (3, 3,filters.shape[-2],filters.shape[-1]))\n",
    "\t\t\n",
    "\t\tconv_layers[l].set_weights([rotated_weights, biases])\n",
    "\n",
    "\n",
    "\treturn model\n",
    "\n",
    "\t\t\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagenet Validation Set location\n",
    "img = \"C:/ILSVRC2012_img_val/ILSVRC2012_val_00000028.JPEG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = glob(\"C:/imagenette2/train/*/*.JPEG\")\n",
    "train_labels = glob(\"C:/imagenette2/train/*/*.JPEG\")\n",
    "train_labels = to_categorical([clsloc[os.path.normpath(str(path)).split(os.path.sep)[-2]] for path in train_labels], num_classes = 1000)\n",
    "\n",
    "\n",
    "trainDS = Dataset.from_tensor_slices((train_list, train_labels))\n",
    "trainDS = (trainDS\n",
    "\t.map(load_images)\n",
    "\t.map(preprocess)\n",
    "\t.cache()\n",
    "\t.batch(1)\n",
    "\t.prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "\n",
    "val_list = glob(\"C:/imagenette2/val/*/*.JPEG\")\n",
    "val_labels = glob(\"C:/imagenette2/val/*/*.JPEG\")\n",
    "val_labels = to_categorical([clsloc[os.path.normpath(str(path)).split(os.path.sep)[-2]] for path in val_list], num_classes = 1000)\n",
    "\n",
    "\n",
    "valDS = Dataset.from_tensor_slices((val_list, val_labels))\n",
    "valDS = (valDS\n",
    "\t.map(load_images)\n",
    "\t.map(preprocess)\n",
    "\t.cache()\n",
    "\t.batch(32)\n",
    "\t.prefetch(AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3925, 1000)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000018B8FF40E88>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000018D781E0408>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000018B9A414F88>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000018B9A416B08>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000018B90159D08>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000018B9013D848>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000018B8921B408>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000018BAE679108>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000018BA2353108>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000018BA23CDCC8>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000018B9A41E308>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000018B90144908>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x0000018B9015E448>\n"
     ]
    }
   ],
   "source": [
    "model_rot = rotate(model, [1], k=2)\n",
    "model_rot.compile(loss=keras.losses.CategoricalCrossentropy(from_logits=False),            \n",
    "\t\t\tmetrics=[\n",
    "\t\t\t\tkeras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "\t\t\t\tkeras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 [==============================] - 36s 281ms/step - loss: 1.1476 - accuracy: 0.7129 - top-5-accuracy: 0.9103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.147555947303772, 0.7128662467002869, 0.9103184938430786]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rot.evaluate(valDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 [==============================] - 12s 96ms/step - loss: 0.6867 - accuracy: 0.8094 - top-5-accuracy: 0.9557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.686688244342804, 0.8094267249107361, 0.9556688070297241]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valDS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
